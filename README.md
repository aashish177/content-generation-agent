# Multi-Agent Content Generation Pipeline

An autonomous AI content creation system that orchestrates 5 specialized agents using LangGraph to produce high-quality, SEO-optimized content. The system leverages Retrieval-Augmented Generation (RAG) with ChromaDB to ground outputs in factual information and brand guidelines.

## Overview

This project implements a stateful, multi-agent workflow that automates the entire content creation process from planning through SEO optimization. Each agent specializes in a specific task and uses vector database retrieval to ensure accuracy and consistency.

### Key Features

- **5 Specialized AI Agents**: Planner, Researcher, Writer, Editor, and SEO Optimizer working in orchestrated sequence
- **RAG-Powered Research**: ChromaDB vector database with 4 specialized collections for semantic search
- **Stateful Workflow**: LangGraph-based orchestration with conditional routing and error handling
- **Context-Aware Generation**: Automated fact verification and style guide compliance
- **Production-Ready Architecture**: Comprehensive error handling, retry logic, and audit trails

## Architecture

### Tech Stack

- **Orchestration**: LangGraph for multi-agent workflow management
- **AI Framework**: LangChain for agent construction
- **LLM**: OpenAI GPT-4o
- **Vector Database**: ChromaDB with persistent storage
- **Embeddings**: OpenAI text-embedding-3-small
- **Language**: Python 3.11+

### Agent Pipeline

```
User Request â†’ Planner â†’ Researcher â†’ Writer â†’ Editor â†’ SEO â†’ Final Content
```

1. **Planner Agent**: Analyzes requests and creates structured content briefs
2. **Researcher Agent**: Queries vector stores and synthesizes findings with citations
3. **Writer Agent**: Generates drafts following brief specifications and research
4. **Editor Agent**: Refines content for style guide compliance and accuracy
5. **SEO Agent**: Optimizes content and generates metadata for search engines

### Vector Store Collections

- `research_docs`: Articles, papers, and factual content
- `writing_samples`: High-performing content templates
- `style_guide`: Brand voice and formatting guidelines
- `seo_data`: Keywords, competitor analysis, and SERP data

## Project Status

### âœ… Completed (Phase 1)

- [x] Core agent implementations (Planner, Researcher, Writer, Editor, SEO)
- [x] Base agent architecture with LLM integration
- [x] ChromaDB vector store manager with 4 collections
- [x] LangGraph workflow with conditional routing
- [x] State management system with TypedDict
- [x] Data ingestion pipeline with text chunking
- [x] Configuration management with environment variables
- [x] Mock data generation for testing

### ðŸš§ In Progress (Phase 2)

- [ ] CLI interface for content generation
- [ ] Complete end-to-end workflow testing
- [ ] Output management (Markdown, JSON, HTML exports)
- [ ] Quality metrics and confidence scoring
- [ ] Comprehensive error handling and logging

### ðŸ“‹ Planned (Phase 3+)

- [ ] REST API with FastAPI
- [ ] Web UI for easier interaction
- [ ] Real-time streaming of agent progress
- [ ] Performance analytics dashboard
- [ ] Batch processing capabilities
- [ ] Human-in-the-loop review points
- [ ] Advanced RAG features (hybrid search, reranking)
- [ ] Multi-language support
- [ ] Integration with external APIs

## Installation

### Prerequisites

- Python 3.11 or higher
- OpenAI API key
- UV package manager (recommended) or pip

### Setup

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd content-generation
   ```

2. **Install dependencies**
   
   Using UV:
   ```bash
   uv sync
   ```
   
   Or using pip:
   ```bash
   pip install -e .
   ```

3. **Configure environment variables**
   
   Create a `.env` file in the project root:
   ```env
   OPENAI_API_KEY=your_openai_api_key_here
   VECTORDB_PATH=./data/vectordb
   ```

4. **Ingest sample data**
   ```bash
   python data/ingest.py
   ```

## Usage

### Current Usage (Development)

```python
from graph.workflow import create_content_workflow

# Create the workflow
app = create_content_workflow()

# Run the pipeline
result = app.invoke({
    "content_request": "Write a comprehensive guide on green tea health benefits",
    "settings": None,
    "retrieved_documents": [],
    "errors": [],
    "agent_logs": []
})

# Access the final content
print(result["final_content"])
print(result["seo_metadata"])
```

### Planned CLI Usage

```bash
# Generate content
python main.py --request "Write a guide on indoor gardening" --output ./outputs

# With custom settings
python main.py --request "Tech trends 2025" --word-count 1500 --tone professional
```

## Project Structure

```
content-generation/
â”œâ”€â”€ agents/              # AI agent implementations
â”‚   â”œâ”€â”€ base.py         # Base agent class
â”‚   â”œâ”€â”€ planner.py      # Planning agent
â”‚   â”œâ”€â”€ researcher.py   # Research agent
â”‚   â”œâ”€â”€ writer.py       # Writing agent
â”‚   â”œâ”€â”€ editor.py       # Editing agent
â”‚   â””â”€â”€ seo.py          # SEO optimization agent
â”œâ”€â”€ graph/              # LangGraph workflow
â”‚   â”œâ”€â”€ state.py        # State management
â”‚   â”œâ”€â”€ workflow.py     # Workflow definition
â”‚   â”œâ”€â”€ nodes.py        # Agent nodes
â”‚   â””â”€â”€ edges.py        # Conditional routing
â”œâ”€â”€ vector_stores/      # Vector database management
â”‚   â””â”€â”€ chroma.py       # ChromaDB manager
â”œâ”€â”€ data/               # Data and vector storage
â”‚   â”œâ”€â”€ ingest.py       # Data ingestion script
â”‚   â””â”€â”€ vectordb/       # ChromaDB persistent storage
â”œâ”€â”€ skills/             # Agent skill documentation
â”œâ”€â”€ outputs/            # Generated content outputs
â”œâ”€â”€ config.py           # Configuration management
â”œâ”€â”€ models.py           # Data models
â””â”€â”€ main.py            # Entry point (WIP)
```

## Configuration

### Agent Temperature Settings

- **Planner**: 0.2 (focused and structured)
- **Researcher**: 0.0 (factual and precise)
- **Writer**: 0.7 (creative and engaging)
- **Editor**: 0.1 (consistent and accurate)
- **SEO**: 0.2 (strategic and analytical)

### RAG Settings

- **Chunk Size**: 1000 tokens
- **Chunk Overlap**: 200 tokens
- **Retrieval K**: 5 documents per query

## Development

### Running Tests

```bash
# Test individual agents
python test_planner.py

# Verify workflow
python verify_workflow.py
```

### Adding New Documents to Vector Store

```python
from vector_stores.chroma import ChromaDBManager
from langchain_core.documents import Document

db = ChromaDBManager()

# Add research documents
docs = [
    Document(
        page_content="Your content here...",
        metadata={"source": "example.txt", "topic": "health"}
    )
]

db.add_documents("research", docs)
```

## Performance Targets

- **End-to-end pipeline**: 2-5 minutes for 1500-word content
- **Planning**: <10 seconds
- **Research**: 20-40 seconds
- **Writing**: 60-120 seconds
- **Editing**: 30-60 seconds
- **SEO**: 15-30 seconds

## Acknowledgments

Built with:
- [LangGraph](https://github.com/langchain-ai/langgraph) for workflow orchestration
- [LangChain](https://github.com/langchain-ai/langchain) for agent framework
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [OpenAI](https://openai.com/) for LLM and embeddings

